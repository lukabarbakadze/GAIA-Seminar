{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Transformer from Scratch — GAIA Week 3 Seminar\n",
    "## Training a Character-Level GPT on ვეფხისტყაოსანი\n",
    "\n",
    "**Live Coding Session**\n",
    "\n",
    "This is a blank template for live coding. We'll build everything from scratch together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download visualization utilities from GitHub\n",
    "!wget -q https://raw.githubusercontent.com/lukabarbakadze/GAIA-Seminar/main/seminar_materials/utils_viz.py\n",
    "\n",
    "# Install dependencies\n",
    "!pip install -q matplotlib seaborn scikit-learn\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Import visualization utilities\n",
    "from utils_viz import (\n",
    "    plot_character_frequency,\n",
    "    plot_attention_heatmap,\n",
    "    plot_gpt_vs_bert_attention,\n",
    "    plot_training_curve,\n",
    "    visualize_embeddings_2d,\n",
    "    print_generation_comparison,\n",
    ")\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "print(\"✅ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Data & Tokenizer\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the dataset\n",
    "!wget -q https://raw.githubusercontent.com/Anbani/anbani.db/master/datasets/vefxistyaosani.txt\n",
    "\n",
    "# Load the dataset\n",
    "with open('vefxistyaosani.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(f\"Dataset: ვეფხისტყაოსანი (The Knight in the Panther's Skin)\")\n",
    "print(f\"Dataset size: {len(text):,} characters\")\n",
    "print(f\"\\nFirst 300 characters:\")\n",
    "print(text[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build character-level tokenizer\n",
    "# TODO: We'll implement this together\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# Create mappings\n",
    "# stoi = ...\n",
    "# itos = ...\n",
    "\n",
    "# Encode and decode functions\n",
    "# def encode(s):\n",
    "#     ...\n",
    "\n",
    "# def decode(l):\n",
    "#     ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the dataset\n",
    "# data = ...\n",
    "\n",
    "# Train/val split\n",
    "# n = int(0.9 * len(data))\n",
    "# train_data = ...\n",
    "# val_data = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Hyperparameters & Data Loading\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 32\n",
    "block_size = 128\n",
    "n_embd = 128\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.2\n",
    "learning_rate = 3e-4\n",
    "max_iters = 5000\n",
    "eval_interval = 250\n",
    "eval_iters = 200\n",
    "\n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading function\n",
    "# def get_batch(split):\n",
    "#     ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Building the Transformer\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single Attention Head\n",
    "class Head(nn.Module):\n",
    "    \"\"\"One head of self-attention.\"\"\"\n",
    "    \n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        # TODO: Implement\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # TODO: Implement\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-Head Attention\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multiple heads of self-attention in parallel.\"\"\"\n",
    "    \n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        # TODO: Implement\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # TODO: Implement\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed-Forward Network\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"Simple feed-forward network.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        # TODO: Implement\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # TODO: Implement\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer Block\n",
    "class Block(nn.Module):\n",
    "    \"\"\"Transformer block: communication followed by computation.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        # TODO: Implement\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # TODO: Implement\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT Language Model\n",
    "class GPTLanguageModel(nn.Module):\n",
    "    \"\"\"Complete GPT language model.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # TODO: Implement\n",
    "        pass\n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "        # TODO: Implement\n",
    "        pass\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0):\n",
    "        # TODO: Implement\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: Training\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model and optimizer\n",
    "# model = GPTLanguageModel()\n",
    "# model = model.to(device)\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "# for iter in range(max_iters):\n",
    "#     ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 5: Generation\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text\n",
    "# context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "# generated = model.generate(context, max_new_tokens=500)\n",
    "# print(decode(generated[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
